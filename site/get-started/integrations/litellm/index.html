
<!doctype html>
<html lang="en" class="no-js">
  <head>
     
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This guide shows how to integrate LiteLLM, an open-source library that simplifies access to 100+ LLMs with load balancing and spend tracking, into kluster.ai.">
      
      
      
        <link rel="canonical" href="https://docs.kluster.ai/get-started/integrations/litellm/">
      
      
        <link rel="prev" href="../langchain/">
      
      
        <link rel="next" href="../msty/">
      
      
      <link rel="icon" href="../../../assets/images/favicon-dark-mode.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.3">
      
    <title>Integrate LiteLLM with kluster.ai | kluster.ai Docs</title>
     
      <link rel="stylesheet" href="../../../assets/stylesheets/main.50c56a3b.min.css">
      
      
  
  
    
    
  
  
  <style>:root{--md-admonition-icon--code:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.6 16.6 4.6-4.6-4.6-4.6L16 6l6 6-6 6-1.4-1.4m-5.2 0L4.8 12l4.6-4.6L8 6l-6 6 6 6 1.4-1.4Z"/></svg>');}</style>



    
<link rel="stylesheet" href="../../../assets/stylesheets/kluster.css" />
<link
  href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@500&display=swap"
  rel="stylesheet"
/>

    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Space+Grotesk:300,300i,400,400i,700,700i%7CSource+Code+Pro:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Space Grotesk";--md-code-font:"Source Code Pro"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/terminal.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/timeline-neoteroi.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-XXX"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-XXX",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-XXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#integrate-litellm-with-klusterai" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
              <button class="md-banner__button md-icon" aria-label="Don't show this again">
                
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
              </button>
            
            
<p>
  Qwen 3 235B is here! kluster.ai now supports it. Try it with
  <a href="/get-started/start-building/real-time/#quickstart-snippets"
    >real-time</a
  >
  or
  <a href="/get-started/start-building/batch/#quickstart-snippets">batch</a>
  inference.
</p>

          </div>
          
            <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <a href="https://docs.kluster.ai/" title="kluster.ai Docs" 
       class="md-header__button md-logo" 
       aria-label="kluster.ai Docs" 
       data-md-component="logo">
      
  <img src="../../../assets/images/logo.png" alt="logo">

    </a>
    
      
        
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
    
    
    
      <li class="md-tabs__item">
        <a href="../../.." class="md-tabs__link">
          
    
      
    
    Home
  
          <div class="framer-1w65mo2" data-framer-name="Bottom Border"></div>
        </a>
      </li>
    
  
      
        
    
    
      
    
    
      
      
        <li class="md-tabs__item md-tabs__item--active">
          <a href="../../get-api-key/" class="md-tabs__link">
            
    
    Get started
  
            <div class="framer-1w65mo2" data-framer-name="Bottom Border"></div>
          </a>
        </li>
      
    
  
      
        
    
    
    
      
      
        <li class="md-tabs__item">
          <a href="../../../api-reference/reference/" class="md-tabs__link">
            
    
    API reference
  
            <div class="framer-1w65mo2" data-framer-name="Bottom Border"></div>
          </a>
        </li>
      
    
  
      
        
    
    
    
      
      
        
    
    
    
      
      
        
    
    
    
      
      
        <li class="md-tabs__item">
          <a href="../../../tutorials/klusterai-api/text-classification/text-classification-openai-api/" class="md-tabs__link">
            
    
    Tutorials
  
            <div class="framer-1w65mo2" data-framer-name="Bottom Border"></div>
          </a>
        </li>
      
    
  
      
    
  
      
    
  
      
    </ul>
  </div>
</nav>
      
    
    <div class="md-header__buttons">
      
      <div class="button-container">
        <div class="framer-1suqwmq-container" data-framer-appear-id="1suqwmq">
          <a class="framer-Bja7l framer-1v7ikbi framer-v-1v7ikbi framer-aotcnd" 
             href="https://platform.kluster.ai/" 
             target="_blank" 
             rel="noopener" 
             data-framer-name="Variant 1">
            <div class="framer-follse" data-framer-name="Gradient Loop Stroke">
              <div data-framer-background-image-wrapper="true">
                <img decoding="async" src="/assets/images/rainbow.gif" alt="">
              </div>
            </div>
            <div class="framer-1a164ql" data-framer-name="Overlay"></div>
            <div class="framer-uvujyo" data-framer-name="BG"></div>
            <div class="framer-p6mefi" data-framer-component-type="RichTextContainer">
              <p class="framer-text">Start building</p>
            </div>
          </a>
        </div>
      </div>
      
        <label class="md-header__button md-icon" for="__search">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
      
    </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
  
  
    
  
  
  <nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
    <label class="md-nav__title" for="__drawer">
      <a href="https://kluster.ai/" title="kluster.ai" class="md-nav__button md-logo" aria-label="kluster.ai" data-md-component="logo">
        
  <img src="../../../assets/images/logo.png" alt="logo">

      </a>
    </label>
    
    <ul class="md-nav__list" data-md-scrollfix>
      
        
        
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

      
        
        
  
  
    
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Get started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../get-api-key/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Get an API key
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Start building
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Start building
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../start-building/setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Initial setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../start-building/real-time/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Real-time inferences
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../start-building/batch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Batch inferences
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../openai-compatibility/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI compatibility
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" checked>
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Integrations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../crewai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CrewAI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../eliza/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    eliza
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../immersive-translate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Immersive Translate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../langchain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LiteLLM
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LiteLLM
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configure-litellm" class="md-nav__link">
    <span class="md-ellipsis">
      Configure LiteLLM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#explore-litellm-features" class="md-nav__link">
    <span class="md-ellipsis">
      Explore LiteLLM features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Explore LiteLLM features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Use streaming responses
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle-multi-turn-conversation" class="md-nav__link">
    <span class="md-ellipsis">
      Handle multi-turn conversation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#put-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Put it all together
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../msty/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Msty
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pydantic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PydanticAI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sillytavern/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SillyTavern
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../typingmind/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TypingMind
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

      
        
        
  
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            API reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api-reference/reference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reference
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

      
        
        
  
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Use cases
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Use cases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Text classification
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_1">
            <span class="md-nav__icon md-icon"></span>
            Text classification
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/text-classification/text-classification-openai-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using OpenAI API
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/text-classification/text-classification-curator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Curator
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/sentiment-analysis-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentiment analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/keyword-extraction-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Keyword extraction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/multiple-tasks-batch-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multiple batch predictions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/model-comparison/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluating LLMs with labeled data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/llm-as-a-judge/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM as a judge
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/finetuning-sent-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-tuning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/image-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/klusterai-api/uploads-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Uploading large files
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

      
    </ul>
  </nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configure-litellm" class="md-nav__link">
    <span class="md-ellipsis">
      Configure LiteLLM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#explore-litellm-features" class="md-nav__link">
    <span class="md-ellipsis">
      Explore LiteLLM features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Explore LiteLLM features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Use streaming responses
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle-multi-turn-conversation" class="md-nav__link">
    <span class="md-ellipsis">
      Handle multi-turn conversation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#put-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Put it all together
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
  
  <article class="md-content__inner md-typeset ">
     


<h1 id="integrate-litellm-with-klusterai">Integrate LiteLLM with kluster.ai<a class="headerlink" href="#integrate-litellm-with-klusterai" title="Permanent link">＃</a></h1>
<p><a href="https://www.litellm.ai/" target="_blank">LiteLLM</a> is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable and cost-effective approach to AI-driven applications.</p>
<p>Integrating LiteLLM with the <a href="https://www.kluster.ai/" target="_blank">kluster.ai</a> API enables the use of kluster.ai's powerful models alongside LiteLLM's flexible orchestration. This combination makes it simple to switch between models on the fly, handle token usage limits with context window fallback, and monitor usage costs in real-time—leading to robust, scalable, and adaptable AI workflows.</p>
<h2 id="prerequisites">Prerequisites<a class="headerlink" href="#prerequisites" title="Permanent link">＃</a></h2>
<p>Before starting, ensure you have the following:</p>
<ul>
<li><strong>A kluster.ai account</strong> - sign up on the <a href="https://platform.kluster.ai/signup" target="_blank">kluster.ai platform</a> if you don't have one</li>
<li><strong>A kluster.ai API key</strong> - after signing in, go to the <a href="https://platform.kluster.ai/apikeys" target="_blank"><strong>API Keys</strong></a> section and create a new key. For detailed instructions, check out the <a href="/get-started/get-api-key/" target="_blank">Get an API key</a> guide</li>
<li><strong><a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/" target="_blank">A python virtual environment</a></strong> - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial</li>
<li>
<p><a href="https://github.com/BerriAI/litellm" target="_blank"><strong>LiteLLM installed</strong></a> - to install the library, use the following command:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>litellm
</code></pre></div>
</li>
</ul>
<h2 id="configure-litellm">Configure LiteLLM<a class="headerlink" href="#configure-litellm" title="Permanent link">＃</a></h2>
<p>In this section, you'll learn how to integrate kluster.ai with LiteLLM. You'll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM's OpenAI-like interface.</p>
<ol>
<li><strong>Import LiteLLM and its dependencies</strong> - create a new file (e.g., <code>hello-litellm.py</code>) and start by importing the necessary Python modules:
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="n">completion</span>
</code></pre></div></li>
<li><strong>Set your kluster.ai API key and Base URL</strong> - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the <a href="/get-started/get-api-key/" target="_blank">Get an API key</a> guide
<div class="highlight"><pre><span></span><code><span class="c1"># Set environment vars, shown in script for readability</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;INSERT_KLUSTER_API_KEY&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;https://api.kluster.ai/v1&quot;</span>
</code></pre></div></li>
<li><strong>Define your conversation (system + user messages)</strong> - set up your initial system prompt and user message. The system message defines your AI assistant's role, while the user message is the actual question or prompt
<div class="highlight"><pre><span></span><code><span class="c1"># Basic Chat</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>   <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of California?&quot;</span><span class="p">}</span>
<span class="p">]</span>
</code></pre></div></li>
<li><strong>Select your kluster.ai model</strong> - choose one of <a href="/get-started/models/" target="_blank">kluster.ai's available models</a> that best fits your use case. Prepend the model name with <code>openai/</code> so LiteLLM recognizes it as an OpenAI-like model request
<div class="highlight"><pre><span></span><code><span class="c1"># Use an &quot;openai/...&quot; model prefix so LiteLLM treats this as an OpenAI-like call</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo&quot;</span>
</code></pre></div></li>
<li><strong>Call the LiteLLM completion function</strong> - finally, invoke the completion function to send your request:
<div class="highlight"><pre><span></span><code><span class="n">response</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></li>
</ol>
<details class="code">
<summary>View complete script</summary>
<div class="highlight"><span class="filename">hello-litellm.py</span><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="n">completion</span>

<span class="c1"># Set environment vars, shown in script for readability</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;INSERT_KLUSTER_API_KEY&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;https://api.kluster.ai/v1&quot;</span>

<span class="c1"># Basic Chat</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>   <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of California?&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Use an &quot;openai/...&quot; model prefix so LiteLLM treats this as an OpenAI-like call</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<p>Use the following command to run your script:</p>
<div class="highlight"><pre><span></span><code><span class="n">python</span> <span class="n">hello</span><span class="o">-</span><span class="n">litellm</span><span class="o">.</span><span class="n">py</span>
</code></pre></div>
<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python hello-litellm.py</span>
    <span data-ty>ModelResponse(id='chatcmpl-9877dfe6-6f1d-483f-a392-d791b89c75d6', created=1739495162, model='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of California is Sacramento.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None)</span>
</div>

<p>That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue to learn how to experiment with more advanced features of LiteLLM.</p>
<h2 id="explore-litellm-features">Explore LiteLLM features<a class="headerlink" href="#explore-litellm-features" title="Permanent link">＃</a></h2>
<p>In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. The following sections demonstrate using LiteLLM's streaming response and multi-turn conversation features with the kluster.ai API.</p>
<p>The following guide assumes you just finished the configuration exercise in the preceding section. If you haven't already done so, please complete the configuration steps in the <a href="#configure-litellm">Configure LiteLLM</a> section before you continue.</p>
<h3 id="use-streaming-responses">Use streaming responses<a class="headerlink" href="#use-streaming-responses" title="Permanent link">＃</a></h3>
<p>You can enable streaming by simply passing <code>stream=True</code> to the <code>completion()</code> function. Streaming returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for-in loop, allowing you to extract the textual content (e.g., <code>chunk.choices[0].delta.content)</code> rather than printing all metadata.</p>
<p>To configure a streaming response, take the following steps:</p>
<ol>
<li>
<p><strong>Update the <code>messages</code> system prompt and first user message</strong> - you can supply a user message or use the sample provided:
<div class="highlight"><pre><span></span><code>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>   <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain the significance of the California Gold Rush.&quot;</span><span class="p">},</span>
    <span class="p">]</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Initiate a streaming request to the model</strong> - set <code>stream=True</code> in the <code>completion()</code> function to tell LiteLLM to return partial pieces (chunks) of the response as they become available rather than waiting for the entire response to be ready
<div class="highlight"><pre><span></span><code>    <span class="c1"># --- 1) STREAMING CALL: Only print chunk text --------------------------------</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response_stream</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># streaming enabled</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error calling model: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--------- STREAMING RESPONSE (text only) ---------&quot;</span><span class="p">)</span>
    <span class="n">streamed_text</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></p>
</li>
<li><strong>Isolate the returned text content</strong> - returning all of the streamed data will include a lot of excessive noise like token counts, etc. You can isolate the text content from the rest of the streamed response with the following code:
<div class="highlight"><pre><span></span><code>    <span class="c1"># Iterate over each chunk from the streaming generator</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response_stream</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="s2">&quot;choices&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
            <span class="c1"># If the content is None, we replace it with &quot;&quot; (empty string)</span>
            <span class="n">partial_text</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
            <span class="n">streamed_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">partial_text</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">partial_text</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># new line after streaming ends</span>
</code></pre></div></li>
</ol>
<h3 id="handle-multi-turn-conversation">Handle multi-turn conversation<a class="headerlink" href="#handle-multi-turn-conversation" title="Permanent link">＃</a></h3>
<p>LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow.</p>
<p>Let's take a closer look at each step:</p>
<ol>
<li><strong>Combine the streamed chunks of the first message</strong> - since the message is streamed in chunks, you must re-assemble them into a single message. After collecting partial responses in <code>streamed_text</code>, join them into a single string called <code>complete_first_answer</code>:
<div class="highlight"><pre><span></span><code>    <span class="c1"># Combine the partial chunks into one string</span>
    <span class="n">complete_first_answer</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">streamed_text</span><span class="p">)</span>
</code></pre></div></li>
<li><strong>Append the assistant's reply</strong> - to enhance the context of the conversation. Add <code>complete_first_answer</code> back into messages under the "assistant" role as follows:
<div class="highlight"><pre><span></span><code>    <span class="c1"># Append the entire first answer to the conversation for multi-turn context</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">complete_first_answer</span><span class="p">})</span>
</code></pre></div></li>
<li><strong>Craft the second message to the assistant</strong> - append a new message object to messages with the user's next question as follows:
<div class="highlight"><pre><span></span><code>    <span class="c1"># --- 2) SECOND CALL (non-streamed): Print just the text ---------------------</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">(</span>
            <span class="s2">&quot;Thanks for that. Can you propose a short, 3-minute presentation outline &quot;</span>
            <span class="s2">&quot;about the Gold Rush, focusing on its broader implications?&quot;</span>
        <span class="p">),</span>
    <span class="p">})</span>
</code></pre></div></li>
<li><strong>Ask the model to respond to the second question</strong> - this time, don't enable the streaming feature. Pass the updated messages to <code>completion()</code> with <code>stream=False</code>, prompting LiteLLM to generate a standard (single-shot) response as follows:
<div class="highlight"><pre><span></span><code>    <span class="k">try</span><span class="p">:</span>
        <span class="n">response_2</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># non-streamed</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error calling model: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>
</code></pre></div></li>
<li><strong>Parse and print the second answer</strong> - extract <code>response_2.choices[0].message["content"]</code>, store it in <code>second_answer_text</code>, and print to the console for your final output: 
<div class="highlight"><pre><span></span><code>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------- RESPONSE 2 (non-streamed, text only) ---------&quot;</span><span class="p">)</span>
    <span class="n">second_answer_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">response_2</span><span class="o">.</span><span class="n">choices</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response_2</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;message&quot;</span><span class="p">):</span>
        <span class="n">second_answer_text</span> <span class="o">=</span> <span class="n">response_2</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">second_answer_text</span><span class="p">)</span>
</code></pre></div></li>
</ol>
<p>You can view the full script below. It demonstrates a streamed response versus a regular response and how to handle a multi-turn conversation.  </p>
<details class="code">
<summary>View complete script</summary>
<div class="highlight"><span class="filename">hello-litellm.py</span><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">litellm.exceptions</span>
<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="n">completion</span>

<span class="c1"># Set environment variables for kluster.ai</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;INSERT_API_KEY&quot;</span>  <span class="c1"># Replace with your key</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;https://api.kluster.ai/v1&quot;</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo&quot;</span>

    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>   <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain the significance of the California Gold Rush.&quot;</span><span class="p">},</span>
    <span class="p">]</span>

    <span class="c1"># --- 1) STREAMING CALL: Only print chunk text --------------------------------</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response_stream</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># streaming enabled</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error calling model: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--------- STREAMING RESPONSE (text only) ---------&quot;</span><span class="p">)</span>
    <span class="n">streamed_text</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Iterate over each chunk from the streaming generator</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response_stream</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="s2">&quot;choices&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
            <span class="c1"># If the content is None, we replace it with &quot;&quot; (empty string)</span>
            <span class="n">partial_text</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
            <span class="n">streamed_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">partial_text</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">partial_text</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># new line after streaming ends</span>

    <span class="c1"># Combine the partial chunks into one string</span>
    <span class="n">complete_first_answer</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">streamed_text</span><span class="p">)</span>

    <span class="c1"># Append the entire first answer to the conversation for multi-turn context</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">complete_first_answer</span><span class="p">})</span>

    <span class="c1"># --- 2) SECOND CALL (non-streamed): Print just the text ---------------------</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">(</span>
            <span class="s2">&quot;Thanks for that. Can you propose a short, 3-minute presentation outline &quot;</span>
            <span class="s2">&quot;about the Gold Rush, focusing on its broader implications?&quot;</span>
        <span class="p">),</span>
    <span class="p">})</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response_2</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># non-streamed</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error calling model: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------- RESPONSE 2 (non-streamed, text only) ---------&quot;</span><span class="p">)</span>
    <span class="n">second_answer_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">response_2</span><span class="o">.</span><span class="n">choices</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response_2</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;message&quot;</span><span class="p">):</span>
        <span class="n">second_answer_text</span> <span class="o">=</span> <span class="n">response_2</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">second_answer_text</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
</details>
<h2 id="put-it-all-together">Put it all together<a class="headerlink" href="#put-it-all-together" title="Permanent link">＃</a></h2>
<p>Use the following command to run your script:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>hello-litellm.py
</code></pre></div></p>
<p>You should see output that resembles the following:</p>
<div id="termynal" data-termynal>
    <span data-ty="input"><span class="file-path">python streaming-litellm.py</span></span>
    <span data-ty>--------- STREAMING RESPONSE (text only) ---------</span>
    <span data-ty>The California Gold Rush, which occurred from 1848 to 1855, was a pivotal event in American history that had significant economic, social, and cultural impacts on the United States and the world. Here are some of the key reasons why the California Gold Rush was important:</span>
    <span data-ty>1. **Mass Migration and Population Growth**: The Gold Rush triggered a massive influx of people to California, with estimates suggesting that over 300,000 people arrived in the state between 1848 and 1852. This migration helped to populate the western United States and contributed to the country's westward expansion.</span>
    <span data-ty>2. **Economic Boom**: The Gold Rush created a huge economic boom, with thousands of people striking it rich and investing their newfound wealth in businesses, infrastructure, and other ventures. The gold rush helped to stimulate economic growth, create new industries, and establish California as a major economic hub.</span>
    <span data-ty>3. **Technological Innovations**: The Gold Rush drove technological innovations, particularly in the areas of mining and transportation. The development of new mining techniques, such as hydraulic mining, and the construction of roads, bridges, and canals, helped to facilitate the extraction and transportation of gold.</span>
    <span data-ty>4. **Impact on Native American Populations**: The Gold Rush had a devastating impact on Native American populations in California, who were forcibly removed from their lands, killed, or displaced by the influx of miners. The Gold Rush marked the beginning of a long and tragic period of colonization and marginalization for Native American communities in</span>
    <span data-ty>--------- RESPONSE 2 (non-streamed, text only) ---------</span>
    <span data-ty>Here's a suggested 3-minute presentation outline on the California Gold Rush, focusing on its broader implications:</span>
    <span data-ty>**Title:** The California Gold Rush: A Catalyst for Change</span>
    <span data-ty>**Introduction (30 seconds)**</span>
    <span data-ty>* Briefly introduce the California Gold Rush and its significance</span>
    <span data-ty>* Thesis statement: The California Gold Rush was a pivotal event in American history that had far-reaching implications for the country's economy, society, and politics.</span>
    <span data-ty>**Section 1: Economic Implications (45 seconds)**</span>
    <span data-ty>* Discuss how the Gold Rush stimulated economic growth and helped establish California as a major economic hub</span>
    <span data-ty>* Mention the impact on trade, commerce, and industry, including the growth of San Francisco and other cities</span>
    <span data-ty>* Highlight the role of the Gold Rush in shaping the US economy and contributing to the country's westward expansion</span>
    <span data-ty>**Section 2: Social and Cultural Implications (45 seconds)**</span>
    <span data-ty>* Discuss the impact of the Gold Rush on Native American populations, including forced removals, violence, and displacement</span>
    <span data-ty>* Mention the diversity of people who came to California during the Gold Rush, including immigrants from China, Latin America, and Europe</span>
    <span data-ty>* Highlight the social and cultural changes that resulted from this diversity, including the growth of cities and the development of new communities</span>
    <span data-ty>**Section 3: Lasting Legacy (45 seconds)**</span>
    <span data-ty>* Discuss the lasting legacy of the Gold Rush, including its contribution to the development of the US West Coast and the growth of the US economy</span>
    <span data-ty>* Mention the ongoing impact of the Gold</span>
</div>

<p>Both responses appear to trail off abruptly, but that's because we limited the output to <code>300</code> tokens each. Feel free to tweak the parameters and rerun the script at your leisure!</p>







  
  



  



 
  </article>
</div>
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../langchain/" class="md-footer__link md-footer__link--prev" aria-label="Previous: LangChain">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                LangChain
              </div>
            </div>
          </a>
        
        
          
          <a href="../msty/" class="md-footer__link md-footer__link--next" aria-label="Next: Msty">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Msty
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 kluster.ai. All rights reserved.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://discord.gg/klusterai" target="_blank" rel="noopener" title="Discord" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485.065 485.065 0 0 0 404.081 32.03a1.816 1.816 0 0 0-1.923.91 337.461 337.461 0 0 0-14.9 30.6 447.848 447.848 0 0 0-134.426 0 309.541 309.541 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.689 483.689 0 0 0-119.688 37.107 1.712 1.712 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.016 2.016 0 0 0 .765 1.375 487.666 487.666 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348.2 348.2 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321.173 321.173 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251.047 251.047 0 0 0 9.109-7.137 1.819 1.819 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.812 1.812 0 0 1 1.924.233 234.533 234.533 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.407 301.407 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391.055 391.055 0 0 0 30.014 48.815 1.864 1.864 0 0 0 2.063.7A486.048 486.048 0 0 0 610.7 405.729a1.882 1.882 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541ZM222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241Zm195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241Z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://twitter.com/klusterai" target="_blank" rel="noopener" title="X" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.linkedin.com/company/kluster-ai/" target="_blank" rel="noopener" title="LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.instagram.com/klusterai/" target="_blank" rel="noopener" title="Instagram" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.facebook.com/profile.php?id=61558614587340" target="_blank" rel="noopener" title="Facebook" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256c0 120 82.7 220.8 194.2 248.5V334.2h-52.8V256h52.8v-33.7c0-87.1 39.4-127.5 125-127.5 16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287v175.9C413.8 494.8 512 386.9 512 256z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["announce.dismiss", "content.code.copy", "navigation.footer", "navigation.tabs", "navigation.tabs.sticky", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
     
      <script src="../../../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
        <script src="../../../js/custom_scroll.js"></script>
      
    
<script>
  const link = document.querySelector("link[rel~='icon']");
  if (link) {
    const isDarkMode = window.matchMedia(
      '(prefers-color-scheme: dark)'
    ).matches;
    isDarkMode
      ? (link.href = "../../../assets/images/favicon-dark-mode.png")
      : (link.href = "../../../assets/images/favicon-light-mode.png");
  }
</script>

  </body>
</html>